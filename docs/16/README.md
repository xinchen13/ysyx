# 性能优化和简易缓存
根据"先完成, 后完美"的系统设计法则, 现在可以讨论如何开展性能优化, 首先需要一套可以指导我们可以回答上述问题的科学方法:

- 评估当前的性能
- 寻找性能瓶颈
- 采用合适的优化方法
- 评估优化后的性能, 对比获得的性能提升是否符合预期
- 性能评估

## 性能评估
需要对性能有一个量化的衡量指标, 而不是凭感觉判断“运行得好不好”. 用这个指标来对当前的系统进行评估, 就是性能优化的第一步. 在通常理解中, "性能高"基本上等同于"跑得快". 因此, 一个直接衡量性能的指标就是程序的执行时间. 所以, 评估一个系统的性能, 就是评估程序在这个系统上的执行时间

### 基准程序选择
要对所有程序进行评估是不现实的, 因此我们需要选择一些具有代表性的程序. 所谓的具有代表性, 就是指优化技术在这些程序上带来的性能收益, 与真实应用场景中的性能收益趋势基本一致.

这里提到了“应用场景”, 说明对于不同的应用场景, 上述趋势很可能不尽相同. 这意味着不同的应用场景需要不同的代表性程序, 这就产生了不同的benchmark. 例如, Linpack用于代表超级计算场景, MLPerf用于代表机器学习训练场景, CloudSuite用于代表云计算场景, Embench用于代表嵌入式场景. 对于通用计算场景, 最著名的benchmark就是SPEC CPU, 用于评估CPU通用计算能力. SPEC(Standard Performance Evaluation Corporation)是一个组织机构, 目标是建立维护用于评估计算机系统的各种标准, 它定义并发布了多种场景下的benchmark. 除SPEC CPU外, 还有面向图形, 工作站, 高性能计算, 存储, 功耗, 虚拟化等各种场景的benchmark. 一个benchmark通常还由若干子项构成. 除了整数测试, SPEC CPU 2006还包含浮点测试, 测试程序覆盖流体力学, 量子化学, 生物分子, 有限元分析, 线性规划, 影像光线追踪, 计算电磁学, 天气预报, 语音识别等不同领域. benchmark也需要与时俱进, 从而代表新时代的程序. 截至2024年, SPEC CPU已经演进了6版, 从一开始的1989年, 1992年, 1995年, 到2000年, 2006年, 最后是最新版本2017年. SPEC CPU 2017加入了一些新程序来代表新的应用场景, 例如生物医学成像, 3D渲染和动画, 采用蒙特卡罗树搜索的人工智能围棋程序(很大概率受2016年AlphaGo的影响)等

CoreMark和Dhrystone属于合成程序(synthetic program), 意思是用若干个代码片段拼接起来的程序. 例如, CoreMark由链表操作, 矩阵乘法和状态机转移操作这三个代码片段组成; Dhrystone则由一些字符串操作的代码片段组成. 合成程序作为benchmark, 最大的问题是其代表性较弱: CoreMark和Dhrystone能代表什么应用场景呢? 相比于SPEC CPU 2006中的各种真实应用, CoreMark中的代码片段顶多只能算C语言的课后作业; Dhrystone就离应用场景更远了, 其代码非常简单(使用短字符串常量), 甚至在现代编译器的作用下, 循环体中的代码片段很可能被深度优化(是否还记得NEMU中的pattern_decode()), 使得评估结果虚高, 无法客观反映系统的性能

对于toy project来说, SPEC CPU的程序有点过于真实了: 一方面它们的规模很大, 即使在x86真机中也需要花费小时量级的时间来运行; 另一方面它们需要运行在Linux环境中, 这意味着我们首先需要设计一个能正确启动Linux的CPU, 然后才能运行SPEC CPU这个benchmark. 我们希望:

- 规模不算太大, 在模拟器甚至在RTL仿真环境中的执行时间不到2小时
- 可在裸机环境中运行, 无需启动Linux
- 程序具有一定代表性, 不像CoreMark和Dhrystone那样采用合成程序

事实上, `am-kernels`中集成的`microbench`就是一个不错的选择. 一方面, `microbench`提供了多种规模的测试集, 模拟器可以采用`ref`规模, RTL仿真环境可以采用`train`规模; 另一方面, `microbench`作为一个AM程序, 无需启动Linux即可运行; 此外, `microbench`包含10个子项, 覆盖排序, 位操作, 语言解释器, 矩阵计算, 素数生成, A*算法, 最大网络流, 数据压缩, MD5校验和等场景. 因此, 下面默认使用`microbench`的`train`规模作为benchmark

如果处理器的应用场景比较明确, 例如运行超级玛丽, 那么还可以直接把超级玛丽作为benchmark, 相当于把“超级玛丽的游戏体验”作为“运行得好”的标准. 和microbench不同, 超级玛丽是一个不会运行结束的程序, 因此可以采用FPS作为量化指标来评估, 而不是采用运行时间

## 寻找性能瓶颈

### 性能公式和优化方向
可以把程序的运行时间分解成以下3个因子:

```
        time      inst     cycle     time
perf = ------- = ------ * ------- * -------
        prog      prog      inst     cycle
```

性能优化的目标是减少程序的运行时间, 也即减小其中的每个因子, 这也揭示了性能优化的三个方向.

#### 程序执行的指令数量(即动态指令数)
- 修改程序, 采用更优的算法.
- 采用更优的编译优化策略. 以gcc为例, 除了使用-O3, -Ofast等通用的优化等级参数外, 还可以对目标程序进行针对性的调参. gcc中与生成代码质量相关的编译选项有大约600多个, 选择合适的编译选项, 能大幅优化程序的动态指令数
- 设计并使用行为更复杂的指令集. 我们知道, CISC指令集包含行为较复杂的指令, 若编译器使用这些复杂指令, 则可以降低动态指令数. 另外, 也可以在处理器中添加自定义的专用指令, 并让程序使用这些专用指令.

#### 平均每条指令执行所需的周期数, CPI, IPC
提升平均每周期所执行的指令数. 这个指标反映了处理器的微结构设计效果, 强大的处理器通过每周期能执行更多的指令. 因此, 通常通过优化处理器的微结构设计来提升IPC, 从而使处理器更快地完成程序的执行. 微结构设计的优化又有不同的方向, 我们将在下文简单讨论

#### 每个周期的时间
提升单位时间内的周期数, 即电路的频率. 可能的优化措施包括:
- 优化数字电路的前端设计, 减小关键路径的逻辑延迟
- 优化数字电路的后端设计, 减小关键路径的走线延迟

如果可以量化地评估上述3个因子, 我们就能更好地评估上述三个优化方向的潜力, 从而指导我们找到性能瓶颈. 幸运的是, 这些指标其实不难获取:
- 动态指令数可在仿真环境中直接统计
- 有了动态指令数, 再统计周期数, 即可计算出IPC
- 电路的频率可查看综合器的报告获取

### 统计IPC
在仿真环境中统计IPC:
- `cycle_count` 统计总时钟周期数, 每一次上升沿计一次数
- `inst_count` 统计总的指令数, 取指的valid每拉高一次计一次数
- 分别计算 IPC 与 CPI

### 简单处理器的性能模型
为了找到性能瓶颈, 我们需要分析IPC受哪些因素的影响. 将处理器划分成前端和后端, 其中, 前端包括取指和译码, 剩余的模块属于后端, 负责真正执行指令并改变处理器状态:

```
       /--- frontend ---\    /-------- backend --------\
                                  +-----+ <--- 2. computation efficiency
                             +--> | FU  | --+
       +-----+     +-----+   |    +-----+   |    +-----+
       | IFU | --> | IDU | --+              +--> | WBU |
       +-----+     +-----+   |    +-----+   |    +-----+
          ^                  +--> | LSU | --+
          |                       +-----+
1. instruction supply                ^
                    3. data supply --+
```

要提升处理器的执行效率, 就需要做到: 

- 处理器前端需要保证指令供给. 如果前端取不到足够的指令, 就无法完全发挥处理器的计算能力. 因为每一条指令的执行都需要先取指, 因此前端的指令供给能力将影响所有指令的执行效率.
- 处理器后端需要保证计算效率和数据供给
    - 对于大部分计算类指令, 其执行效率取决于相应功能单元的计算效率. 例如, 乘除法指令的执行效率还受乘除法器的计算效率的影响. 类似的还有浮点执行和浮点处理器单元FPU等
    - 对于访存类指令, 其执行效率取决于LSU的访存效率. 特别地, 对于load指令, 处理器需要等待存储器返回数据, 然后才能将数据写回寄存器堆. 这意味着, load指令的执行效率取决于LSU和存储器的数据供给能力. store指令则比较特殊, 因为store指令不需要写入寄存器堆, 原则上处理器不必等待数据完全写入存储器. 在高性能处理器中, 通常会设计一个store buffer部件, 处理器将store指令的信息写入store buffer后, 即认为store执行结束, 后续由store buffer部件控制将数据真正写入存储器中. 当然, 这增加了处理器设计的复杂度, 例如load指令还需要检查最新的数据是否在store buffer中
    
那么, 我们应该如何量化地评估处理器的指令供给, 计算效率和数据供给呢? 换句话说, 我们真正想了解的是处理器在运行指定的benchmark时, IFU和LSU等模块有没有全速工作. 为此, 我们又需要收集更多的信息.

### 性能事件和性能计数器
为了量化地评估处理器的指令供给, 计算效率和数据供给, 我们需要进一步理解影响它们的细致因素. 以指令供给为例, 能直接反映指令供给能力的, 就是IFU是否取到了指令. 为此, 我们可以把"IFU取到指令"看作一个事件, 来统计这个事件发生的频次, 如果这个事件经常发生, 指令供给能力就强; 否则, 指令供给能力就弱. 这些事件称为性能事件(performance event), 通过它们, 我们可以将性能模型中一些较为抽象的性能指标转化为电路上的具体事件. 类似地, 我们还可以统计"LSU取到数据"这个事件发生的频次, 来衡量数据供给能力的强弱; 统计"EXU完成计算"这个事件发生的频次, 来衡量计算效率的高低.

要统计性能事件发生的频次, 我们只需要在硬件中添加一些计数器, 在检测到性能事件发生时, 计数器的值就加1. 这些计数器称为性能计数器(performance counter). 有了性能计数器, 我们就可以观察"程序在处理器上运行的时间都花在哪里"了, 相当于对处理器内部做 profiling. 要在电路上检测性能事件的发生并不难, 我们可以利用总线机制的握手信号进行检测. 例如, IFU取指的R通道握手时, 表示IFU接收到AXI总线返回的数据, 从而完成一次取指操作, 因此当R通道握手时, 我们就可以让相应的性能计数器加1.

### 添加性能计数器
在NPC中添加一些性能计数器, 至少包含如下性能事件的性能计数器:

- IFU取到指令
- LSU取到数据
- 译码出各种类别的指令, 如计算类指令, 访存指令, CSR指令等

实现上在处理器内部添加一个计数器模块pmu，对事件进行统计，并通过宏来控制是否例化; 在仿真环境中实现程序执行完后收集pmu的计数器值并打印; 实现后运行microbench的test规模, 收集性能计数器的结果, 如果实现正确, 语义相近的不同性能计数器的统计结果应存在一致性. 例如译码得到的不同类别的指令的总数, 应与IFU取到指令的数量一致, 也与动态指令数一致

相比于事件发生, 有时候我们更关心事件什么时候不发生, 以及为什么不发生. 例如, 其实我们更关心IFU什么时候取不到指令, 以及IFU为什么取不到指令, 梳理其中的缘由有助于我们理解指令供给的瓶颈在哪里, 从而为提升处理器的指令供给提供指导. 我们可以把"事件不发生"定义成一个新的事件, 并为新事件添加性能计数器:

- 每种类别的指令占多少比例? 它们各自平均需要执行多少个周期?
- IFU取不到指令的原因有哪些? 这些原因导致IFU取不到指令的几率分别是多少: (当前原因只有后续译码执行访存卡住)
- LSU的平均访存延迟是多少: (读60个cycle, 写30个cycle)

性能计数器的trace: 前面介绍的性能计数器的使用方式都是在仿真结束后输出并分析. 如果我们每周期都输出性能计数器的值, 我们就能得到性能计数器的trace, 借助一些绘图工具(如python的matplotlib绘图库), 我们可以绘制出性能计数器的值随时间变化的曲线, 将仿真过程中性能计数器的变化过程可视化, 从而帮助我们更好地判断性能计数器的变化过程是否符合预期

### 阿姆达尔定律 (Amdahl's law)
性能计数器可以为处理器微结构的优化提供量化指导. 那么, 性能瓶颈究竟在哪里? 哪些优化工作是值得做的呢? 优化工作的预期性能收益是多少? 我们需要在开展具体的优化之前就回答这些问题, 以帮助我们规避那些预期性能收益很低的优化工作, 从而将更多时间投入到收益高的优化工作中. 这听上去像一个预测未来的工作, 但Amdahl's law可以告诉我们答案

```
The overall performance improvement gained by optimizing a single part
of a system is limited by the fraction of time that the improved part
is actually used.

优化系统中某部分所获得的总体性能收益, 受限于那部分实际使用的时间占比
```

假设系统某部分实际使用的时间占比是`p`, 该部分在优化后的加速比是`s`, 则整个系统的加速比为`f(s) = 1 / (1 - p + p / s)`, 这就是Amdahl's law的公式表示

#### 根据性能计数器寻找合适的性能瓶颈
根据性能计数器的统计结果, 尝试挖掘一些潜在的优化对象(以 test 规模的 microbench 为例, **优化方案的选择一定要基于负载的运行情况**):

```
[/home/xinchen/ysyx/npc/csrc/tiktok.c:52 pmu_display] ************ Performance Monitor ************
[/home/xinchen/ysyx/npc/csrc/tiktok.c:53 pmu_display] Total cycle count = 13998424
[/home/xinchen/ysyx/npc/csrc/tiktok.c:54 pmu_display]    - A(alu) type count         = 603912(0.043)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:55 pmu_display]    - B(branch) type count      = 267790(0.019)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:56 pmu_display]    - C(csr) type count         = 1(0.000)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:57 pmu_display]    - Memory load type count    = 4384117(0.313)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:58 pmu_display]    - Memory store type count   = 1735473(0.124)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:59 pmu_display]    - Front end: fetch count    = 6439823(0.460)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:60 pmu_display] Total insts count = 567308
[/home/xinchen/ysyx/npc/csrc/tiktok.c:61 pmu_display]    - A(alu) type count         = 301956(0.532)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:62 pmu_display]    - B(branch) type count      = 133895(0.236)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:63 pmu_display]    - C(csr) type count         = 1(0.000)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:64 pmu_display]    - Memory load type count    = 72022(0.127)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:65 pmu_display]    - Memory store type count   = 59434(0.105)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:66 pmu_display] CPI = 24.675 (IPC = 0.040527)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:67 pmu_display]    - A(alu) type         = 2.000
[/home/xinchen/ysyx/npc/csrc/tiktok.c:68 pmu_display]    - B(branch) type      = 2.000
[/home/xinchen/ysyx/npc/csrc/tiktok.c:69 pmu_display]    - C(csr) type         = 1.000
[/home/xinchen/ysyx/npc/csrc/tiktok.c:70 pmu_display]    - Memory load type    = 60.872
[/home/xinchen/ysyx/npc/csrc/tiktok.c:71 pmu_display]    - Memory store type   = 29.200
[/home/xinchen/ysyx/npc/csrc/tiktok.c:73 pmu_display] *********************************************
```
- 从时间占比上来看, 优化取指和 lsu load 是收益最大的
- 利用Amdahl's law估算它们能获取的理论收益:
    1. 优化取指的理论收益: `1/(1-0.460)=1.85`
    2. 优化laod的理论收益: `1/(1-0.313)=1.46`

### 校准访存延迟
如果仿真环境的行为和真实芯片越接近, 评估结果的误差就越小, 在性能计数器的指导下开展的优化所取得的性能提升就越真实: 之前的ysyxSoC环境是假设处理器和各种外设运行在同一频率下: verilator仿真的一个周期, 既是处理器中的一个周期, 也是外设中的一个周期. 但实际上并非如此: 受电气特性的影响, 外设通常只能运行在低频率, 例如SDRAM颗粒通常只能运行在100MHz左右, 过高的频率会导致时序违例, 使得SDRAM颗粒无法正确工作; 但另一方面, 使用先进工艺的处理器通常能够运行在更高的频率.

为了获得更准确的仿真结果, 以指导我们进行更有效的优化, 我们需要对访存延迟进行校准(calibration). 校准的方式有两种, 一种是使用支持多个时钟域的仿真器, 例如VCS或ICARUS verilog. 和采用周期精确模型方式实现的verilator不同, 这种仿真器采用事件队列模型的方式实现, 把Verilog中的每次计算都看作事件, 并且可以维护事件的延迟, 从而可以正确地维护多时钟域中不同模块在不同频率下工作时每次计算的顺序. 不过为了维护事件队列模型, 这种仿真器的运行速度通常要低于verilator

校准的第二种方式是修改RTL代码, 在ysyxSoC中插入一个延迟模块, 负责将请求延迟若干周期, 来模拟设备在低频率运行的效果, 使得NPC等待的周期数与其将来在高频率运行时所等待的周期数接近. 这种方式的实现不算复杂, 而且可以使用较快的verilator来仿真, 我们选择这种方式. 此外, 这种方式也适用于FPGA. 延迟模块需要等待的周期数与设备服务请求所花费的时间有关, 并不是一个固定的常数, 因此需要在延迟模块中动态计算. 假设请求在设备中花费了k个周期, 处理器和设备的频率比是r(应有r >= 1), 那么延迟模块中需要计算出处理器所需等待的周期数`c = k * r`. 为了进行这一动态计算, 我们又需要考虑两个问题:

1. 如何低开销地实现乘法
2. 如果r是小数, 如何实现小数的乘法

例如yosys-sta项目报告的频率是550MH, 那么`r = 550 / 100 = 5.5`, 但如果把5.5按5来计算, 一个在设备端花费6周期请求将会在处理器端引入3个周期的误差, 对高速运行的CPU来说误差太大, 误差的积累会明显地影响性能计数器的值, 从而进一步影响优化的决策. 考虑到ysyxSoC的代码不参与综合和流片, 其实我们可以用一些简单的方法解决问题, 例如用`*`来计算乘法的结果, 用定点数来表示小数

首先我们先考虑r是整数时, 如何实现乘法. 既然延迟模块本身也需要等待设备的回复, 等待的时间正好是请求在设备中花费的周期数k, 那干脆让延迟模块在等待的每个周期中对一个计数器进行累加, 每周期加r即可. 对于给定的处理器频率和设备频率, r是个定值, 因此可以直接硬编码到RTL代码中. 在延迟模块收到设备的回复后, 就进入等待状态, 每周期让计数器减1, 减到0时再将请求回复给上游.

然后我们来考虑r是小数的情况. 既然小数部分不方便处理, 直接截断又会引入较大误差, 我们可以想办法将小数部分并入整数部分进行累加. 事实上, 我们可以引入一个放大系数s, 累加时每周期往计数器加`r * s`, 即累加结束时, 计数器的值为`y = r * s * k`, 然后在进入等待状态前, 将计数器更新为`y / s`即可. 因为s是一个常数, 因此`r * s`的结果也可以直接硬编码到RTL中, 当然这里的`r * s`很有可能还不是整数, 这里我们将其截断为整数, 虽然这理论上这仍然会引入一定的误差, 但我们可以证明误差比之前小很多. 不过y的值是动态计算的, 不能硬编码到RTL中, 因此对于一般的s, `y / s`需要计算除法. 你应该很快想到, 我们可以取一些特殊的s, 来简化这一计算的过程! 通过这种方式, 我们可以把误差减少到原来的`1/s`, 即原来在累加阶段累积的误差达到s时, 在这种新方法下的误差才增加1

#### 实现校准访存延迟
回顾当前的ysyxSoC, 其中SDRAM采用APB接口, 因此我们需要实现一个APB的延迟模块. ysyxSoC中已经包含一个APB延迟模块的框架, 并集成到APB Xbar的上游, 可捕捉所有APB访问请求, 包括SDRAM的访问请求:

- 在`ysyxSoC/perip/amba/apb_delayer.v`中实现相应代码
- 为了实现APB延迟模块, 根据APB协议的定义, 梳理出一个APB事务何时开始, 何时结束. 假设一个APB事务从`t0`时刻开始, 设备端在`t1`时刻返回APB的回复, APB延迟模块在`t1'`时刻向上游返回APB的回复, 则应有等式`(t1 - t0) * r = t1' - t0`
- 关于r的取值, 假设设备运行在100MHz的环境下, 可以根据yosys-sta的综合报告计算出r. 至于s, 理论上当然是越大越好, 不过你只要选择一个实际中够用的s即可(实际选择了10). 实现后, 尝试取不同的r, 在波形中观察上述等式是否成立

#### 重新寻找优化瓶颈
添加延迟模块后, 重新运行测试并收集性能计数器的统计结果, 然后根据Amdahl's law寻找性能瓶颈:

- 依然采用 test 规模的 microbench, core 频率假设为 1GHz, device 频率假设为 100MHz
- 从时间占比上来看, 优化取指和 lsu load 是收益最大的
- 利用Amdahl's law估算它们能获取的理论收益:
    1. 优化取指的理论收益: `1/(1-0.418)=1.71`
    2. 优化laod的理论收益: `1/(1-0.420)=1.72`

#### 评估NPC的性能
添加延迟模块后, 运行 microbench 的 train 规模测试, 记录各种性能数据, 包括主频信息和各种性能计数器. 校准访存延迟后, 在ysyxSoC中运行microbench的train规模测试预计需要花费数小时, 但我们将得到与流片环境非常接近的性能数据. 后续可以在每次添加一个特性后, 就重新评估并记录性能数据, 来帮助梳理每一个特性带来的性能收益. 记录的数据位于 [performance-record](../performance-record.md)

#### 提升功能测试的效率
将校准访存延迟后的ysyxSoC仿真环境用于性能评估是很合适的, 但这一环境的仿真效率明显低于之前的 riscv32e-ysyxsoc: 在注释 `vsrc/inc/defines.svh` 中 `PMU_ON` 和 `csrc/common.h` 与 `CONFIG_PMU` 后, 即可用于功能测试

## 经典体系结构的4类优化方法
1. 局部性: 利用数据访问的性质提升指令供给和数据供给能力. 代表性技术是缓存
2. 并行: 多个实例同时工作, 提升系统整体的处理能力. 并行方法又有很多分类:
    - 指令级并行: 同时执行多条指令, 相关技术包括流水线, 多发射, VLIW和乱序执行
    - 数据级并行: 同时访问多个数据, 相关技术包括SIMD, 向量指令/向量机
    - 任务级并行: 同时执行多个任务, 相关技术包括多线程, 多核, 多处理器和多进程; GPU属于SIMT, 是一种介于数据级并行和任务级并行之间的并行方法
3. 预测: 在不知道正确选择时, 先投机地执行一个选择, 在后续检查选择是否正确, 如果预测正确, 就能降低等待的延迟, 从而获得性能收益. 如果预测错误, 就需要通过额外的机制进行恢复. 代表性技术包括分支预测和缓存预取
4. 加速器: 使用专门的硬件部件来执行特定任务, 从而提升该任务的执行效率, 一些例子包括:
    - AI加速器: 对AI负载的计算过程进行加速, 通常通过总线访问
    - 自定义扩展指令: 将加速器集成到CPU内部, 通过通过新增的自定义扩展指令来访问
    - 乘除法器: 可以视为一类加速器, 将RVM视为RVI的扩展, 通过乘除法指令控制专门的乘除法硬件模块, 来加速乘除法的计算过程

#### 合格的处理器架构师应该具备如下能力:
- 理解程序如何在处理器上运行
- 对于支撑程序运行的特性, 能判断它们适合在硬件层次实现, 还是适合在软件层次实现
- 对于适合在硬件层次实现的特性, 能提出一套在各种因素的权衡之下仍然满足目标要求的设计方案

## 存储层次结构和局部性原理
校准ysyxSoC的访存延迟后, 发现性能瓶颈在于指令供给: 取一条指令都要等待数十上百个周期, 流水线根本没法流水. 要提升指令供给能力, 最合适的就是使用缓存技术. 我们需要先了解计算机的存储层次结构和局部性原理

### 存储层次结构 (Memory Hierarchy)
计算机中存在不同的存储介质, 如寄存器, 内存, 硬盘和磁带, 它们具有不同的物理特性, 因而各种指标也有所不同. 可以从访问时间, 容量和成本这几个方面评估它们. 由于存储介质物理特性的限制, 没有一种存储器能同时满足容量大, 速度快, 成本低等各种指标. 因此, 计算机通常集成多种存储器, 并通过一定的技术将它们有机组织起来, 形成存储层次结构, 在整体上达到容量大, 速度快, 成本低的综合指标. 这听上去有点不可思议, 不过关键在于如何把各种存储器有机组织起来

```
access time     /\        capacity    price
               /  \
   ~1ns       / reg\        ~1KB     $$$$$$
             +------+
   ~10ns    /  DRAM  \      ~10GB     $$$$
           +----------+
   ~10ms  /    disk    \    ~1TB       $$
         +--------------+
   ~10s /      tape      \  >10TB       $
       +------------------+
```

### 局部性原理
其实上述的组织方式是有讲究的, 其中的奥秘就是程序的局部性原理. 计算机架构师发现, 程序在一段时间内对存储器的访问通常集中在一个很小的范围: 
- 时间局部性: 访问一个存储单元后, 短时间内可能再次访问它
- 空间局部性: 访问一个存储单元后, 短时间内可能访问它的相邻存储单元

上述现象与程序的结构和行为有关:
- 程序大多数时候将顺序执行或循环执行, 二者分别遵循空间局部性和时间局部性
- 编写程序时, 相关的变量在源代码中的位置相距不远, 或者采用结构体来组织, 编译器也会为其分配分配相近的存储空间, 从而呈现出空间局部性
- 程序执行过程中访问变量的次数通常不小于变量的数量(否则将存在未被使用的变量), 因此必定有变量会被多次访问, 从而呈现出时间局部性
- 对于数组, 程序通常使用循环来遍历, 从而呈现出空间局部性

局部性原理告诉我们, 程序对存储器的访问表现出集中的特性. 这说明, 即使慢速存储器的容量很大, 程序在一段时间内只会访问很小的一部分数据. 既然如此, 我们可以先将这部分数据从慢速存储器中搬运到快速存储器中, 然后在快速存储器中访问它们.

这就是存储层次结构中各种存储器之间组织方式的诀窍: 将各种存储器按层次排列, 上层存储器速度快但容量小, 下层存储器容量大但速度慢; 访问数据时, 先访问速度较快的上层存储器, 如果数据在当前层级(称为命中), 则直接访问当前层级的数据; 否则(称为缺失), 就在下一层级寻找, 下层将目标数据及其相邻数据传递给上层. 其中, "把目标数据传递给上层存储器"利用了时间局部性, 期望下次访问目标数据时能在速度快的存储器中命中; 而"把相邻数据传递给上层存储器"则利用了空间局部性, 期望下次访问相邻数据时也能在速度快的存储器中命中.

例如, 在访问DRAM时, 如果数据不存在, 则访问机械硬盘, 并将目标数据及其相邻数据搬运到DRAM中, 下次访问这些数据时, 即可在DRAM中命中, 从而直接访问DRAM中数据. 通过这种方式, 我们近似得到了一个访问速度接近DRAM, 容量接近机械硬盘的存储器! 在成本方面, 以上文电商平台的报价为例, 一根16GB的内存条和一块4TB的机械硬盘的总价格不到900元, 但如果要采购4TB的内存条, 则需要329 * (4TB / 16GB) = 84224元

当然, 天下没有免费的午餐, 要实现上文的效果是有条件的, 计算机系统的设计需要满足局部性原理: 一方面, 计算机系统需要设计并实现存储层次结构; 另一方面, 程序员也需要开发出局部性较好的程序, 使其在存储层次结构中能获得较好的性能. 如果程序的局部性较差, 访问的数据不具备集中的特性, 将导致大部分访问都无法在快速存储器中命中, 从而使得整个系统的表现接近于访问慢速存储器

## 简易缓存
### 缓存介绍
根据瓶颈分析结果, 为了优化指令供给, 实际上我们需要做的是提升访问DRAM的效率. 为此, 我们只需要按照计算机存储层次结构的思想, 在寄存器和DRAM之间添加一层存储器即可. 这就是缓存(cache)的思想. 也即, 在访问DRAM之前, 先访问cache, 若命中, 则直接访问; 若缺失, 则先将数据从DRAM读入cache, 然后再访问cache中的数据.

谁来完成上下两个存储层次之间的数据搬运? 取决于谁能访问这两个存储层次. 计算机中的主体只有软件程序和硬件电路, 而软件程序的本质是指令序列. 虽然指令可以访问DRAM, 但指令集手册中定义的编程模型通常不包含cache, 也就是说从功能上来说, cache对软件程序不可见, 因此只能是硬件电路来完成上述读入操作(本质上还是状态机). 而对于DRAM和磁盘这两个存储层次, 指令可以访问DRAM, 也可以通过MMIO访问作为设备的磁盘, 因此可以由软件负责将数据从磁盘读入DRAM, 上文提到的操作系统就是如此. 当然, 原则上你也可以设计一个硬件模块, 专门完成DRAM和磁盘之间的数据搬运工作, 但这个硬件模块通常只能适配一种磁盘, 其灵活性远不如操作系统中的驱动程序.

cache中存放的数据块称为cache块(cache行, cache line). 自然地, 设计cache需要关注如下问题:
- cache line的大小应该是多少
- 如何检查访存请求是否在cache中命中
- cache的容量通常比DRAM小, 如何维护cache块和DRAM中数据块之间的映射关系, cache满了后怎么办
- CPU可能会执行写操作, 从而更新数据块中的数据, cache应如何维护

### 简易指令缓存
考虑指令缓存(instruction cache, 简称icache), 由于IFU的取指过程无需写入内存, 因此icache是只读的, 我们可以先不考虑如何处理CPU写入数据块的情况. 至于块大小, 我们先取一条指令的长度, 即4B. 这也许不是一个最好的设计, 但对icache来说, 小于4B肯定不是一个好的设计, 否则取出一条新指令时, 还需要进行多次访存. 至于大于4B是否更好, 我们后面再来评估.

为了检查访存请求是否在cache中命中, 很自然地, 除了存储数据块本身, cache还需要记录块的一些属性. 最直接的方式就是记录这个块的一种唯一编号, 不过我们还希望, 这个唯一编号的计算方式要足够简单. 既然数据块是从内存来的, 我们可以按块大小对内存进行编号, 内存地址`addr`对应的数据块的编号即为`addr / 4`, 这个编号称为块的标签(tag). 这样, 我们只要计算出访存地址的tag, 然后和每个cache块的tag对比, 就能知道目标块是否在cache中.

然后来考虑cache块的组织问题. 根据存储层次结构, cache的容量不可能和DRAM一样大, 通常也不可能小到只有1个cache块, 因此需要考虑读入一个新块时, 应该将其读入到哪个cache块中. 由于cache中有多个cache块, 因此我们也可以给cache块进行编号. 最简单的组织方式就是将一个新块读入到固定的cache块中, 这种组织方式称为直接映射(direct-mapped). 为此, 我们需要明确从内存地址addr到cache块号的映射关系. 假设cache可存放k个cache块, 一种简单的映射关系是cache块号为 `(addr / 4) % k`, 也即, 对于内存地址为`addr`的数据块, 它将被读入编号为`(addr / 4) % k`的cache块.

显然, 多个数据块可能会映射到相同的cache块, 这时需要决定应该保留已有的cache块, 还是往cache块中读入新块. 根据程序的局部性原理, 将来访问新块的概率更大, 因此读入新块时, 应该将已有的cache块替换为新块, 使得接下来一段时间内访问新块时能都在cache中命中.

我们可以把所有cache块看作一个数组, cache块号就是数组的索引(index), 因此cache块号也称为块索引. 对于块大小是b字节, 共k个cache块的直接映射cache, 有`tag = addr / b`, `index = (addr / b) % k`. 为了方便计算, 通常取b和k为2的幂, 假设`b = 2^m`, `k = 2^n`. 假设addr为32位, 则有`tag = addr / 2^m = addr[31:m]`, `index = (addr / 2^m) % 2^n = addr[m+n-1:m]`, 可见, index实际上是tag中的低n位. 由于在直接映射的cache中, index不同的数据块必定会被映射到不同的cache块中, 即使它们的tag的高m位(即`addr[31:m+n]`)相同, 因此, 记录tag时只需要记录`addr[31:m+n]`即可.

一个访存地址可以划分成以下3部分: tag, index, offset. 其中tag部分作为数据块在cache中的唯一编号, index部分作为数据块在cache中的索引, offset部分属于块内偏移, 指示需要访问数据块中的哪部分数据. 最后, 在系统复位时, cache中无任何数据, 此时所有cache块均无效. 为了标识一个cache块是否有效, 需要为每个cache块添加一个有效位(valid). valid和tag统称为cache块的元数据(metadata), 其含义是用于管理数据的数据, 此处被管理的数据就是cache块.

```
 31    m+n m+n-1   m m-1    0
+---------+---------+--------+     +-------+
|   tag   |  index  | offset |     | valid |
+---------+---------+--------+     +-------+
```

综上, 上述icache的工作流程如下:

1. IFU向icache发送取指请求
2. icache获得取指请求的地址后, 根据index部分索引出一个cache块, 判断其tag与请求地址的tag是否相同, 并检查该cache块是否有效. 若同时满足上述条件, 则命中, 跳转到第5步
3. 通过总线在DRAM中读出请求所在的数据块
4. 将该数据块填入相应cache块中, 更新元数据
5. 向IFU返回取出的指令

上述工作流程包含了一次总线的访问, 因此icache的实现也可以看成是总线状态机的扩展

#### 实现icache
根据上述流程, 实现一个简单的icache, 块大小为4B, 共16个cache块. 一般来说, cache的存储阵列(包括数据和元数据)都通过SRAM来实现, 但在ASIC流程中使用SRAM涉及到选型和实例化, 其中SRAM的选型可能会影响到数据和元数据的存放. 为简单起见, 此处先通过寄存器来实现存储阵列, 提高实现的灵活性.

- 实现时, 将相关参数实现成可配置的, 以便于后续评估不同配置参数的性能表现
- 访问 cache 和访问内存实现成两个状态机器, 一个是另一个的子状态机
- 实现后, 评估性能表现: 见性能评估表 (两次测试指令数不同猜测原因是与计时有关)
- 并不是所有的地址空间都适合缓存, 只有存储器类型的地址空间才适合缓存. 此外, SRAM的访问延迟本身就只有1周期, 因此也无需缓存, 将缓存块留给其他的地址空间是一个更合适的方案

#### 估算dcache的理想收益
通常, LSU也有与其配对的缓存, 称为数据缓存(data cache, 简称dcache). 在实现dcache之前, 我们可以先估计它在理想情况下的性能收益. 假设dcache的容量无限大, 访问dcache的命中率为100%, 且dcache的访问延迟为1周期, 尝试根据性能计数器估算添加这样一个dcache带来的性能收益

## 形式化验证
略

## 缓存的优化
由于缓存技术主要用于提升访存效率, 因此很自然地, 我们应该通过访存相关的指标来评价缓存的性能表现. 通常通过`AMAT(Average Memory Access Time)`来评估缓存的性能表现, 假设缓存的命中率为`p`:

```
AMAT    = p * access_time + (1 - p) * (access_time + miss_penalty)
        = access_time + (1 - p) * miss_penalty
```

其中`access_time`为cache的访问时间, 即从cache接收访存请求到得出命中结果所需的时间, `miss_penalty`为cache缺失时的代价, 此处即访问DRAM的时间.

这个等式给优化缓存的性能表现提供了指导方向: 减少访问时间`access_time`, 提升命中率p, 或者减少缺失代价`miss_penalty`. 在目前的NPC中, 访问时间在架构设计上能优化的空间不多, 更多是受具体实现的影响, 如周期数和关键路径. 因此, 后续我们重点讨论命中率和缺失代价的优化

### 统计AMAT
在NPC中添加合适的性能计数器, 统计icache的AMAT.

### 优化命中率
要提升命中率, 也就是要降低缺失率, 为此, 我们需要先了解cache的缺失有哪些原因

cache缺失的3C模型:
- Compulsory miss, 强制缺失, 定义为在一个容量无限大的cache中所发生的缺失, 表现为在第一次访问一个数据块时所发生的缺失
- Capacity miss, 容量缺失, 定义为不扩大cache容量就无法消除的缺失, 表现为因cache无法容纳所有所需访问的数据而发生的缺失
- Conflict miss, 冲突缺失, 定义为除上述两种原因外引起的缺失, 表现为因多个cache块之间相互替换而发生的缺失

有了3C模型, 我们就可以为每种类型的缺失提出针对性的方案, 来降低相应的缺失率了.