# 性能优化和简易缓存
根据"先完成, 后完美"的系统设计法则, 现在可以讨论如何开展性能优化, 首先需要一套可以指导我们可以回答上述问题的科学方法:

- 评估当前的性能
- 寻找性能瓶颈
- 采用合适的优化方法
- 评估优化后的性能, 对比获得的性能提升是否符合预期
- 性能评估

## 性能评估
需要对性能有一个量化的衡量指标, 而不是凭感觉判断“运行得好不好”. 用这个指标来对当前的系统进行评估, 就是性能优化的第一步. 在通常理解中, "性能高"基本上等同于"跑得快". 因此, 一个直接衡量性能的指标就是程序的执行时间. 所以, 评估一个系统的性能, 就是评估程序在这个系统上的执行时间

### 基准程序选择
要对所有程序进行评估是不现实的, 因此我们需要选择一些具有代表性的程序. 所谓的具有代表性, 就是指优化技术在这些程序上带来的性能收益, 与真实应用场景中的性能收益趋势基本一致.

这里提到了“应用场景”, 说明对于不同的应用场景, 上述趋势很可能不尽相同. 这意味着不同的应用场景需要不同的代表性程序, 这就产生了不同的benchmark. 例如, Linpack用于代表超级计算场景, MLPerf用于代表机器学习训练场景, CloudSuite用于代表云计算场景, Embench用于代表嵌入式场景. 对于通用计算场景, 最著名的benchmark就是SPEC CPU, 用于评估CPU通用计算能力. SPEC(Standard Performance Evaluation Corporation)是一个组织机构, 目标是建立维护用于评估计算机系统的各种标准, 它定义并发布了多种场景下的benchmark. 除SPEC CPU外, 还有面向图形, 工作站, 高性能计算, 存储, 功耗, 虚拟化等各种场景的benchmark. 一个benchmark通常还由若干子项构成. 除了整数测试, SPEC CPU 2006还包含浮点测试, 测试程序覆盖流体力学, 量子化学, 生物分子, 有限元分析, 线性规划, 影像光线追踪, 计算电磁学, 天气预报, 语音识别等不同领域. benchmark也需要与时俱进, 从而代表新时代的程序. 截至2024年, SPEC CPU已经演进了6版, 从一开始的1989年, 1992年, 1995年, 到2000年, 2006年, 最后是最新版本2017年. SPEC CPU 2017加入了一些新程序来代表新的应用场景, 例如生物医学成像, 3D渲染和动画, 采用蒙特卡罗树搜索的人工智能围棋程序(很大概率受2016年AlphaGo的影响)等

CoreMark和Dhrystone属于合成程序(synthetic program), 意思是用若干个代码片段拼接起来的程序. 例如, CoreMark由链表操作, 矩阵乘法和状态机转移操作这三个代码片段组成; Dhrystone则由一些字符串操作的代码片段组成. 合成程序作为benchmark, 最大的问题是其代表性较弱: CoreMark和Dhrystone能代表什么应用场景呢? 相比于SPEC CPU 2006中的各种真实应用, CoreMark中的代码片段顶多只能算C语言的课后作业; Dhrystone就离应用场景更远了, 其代码非常简单(使用短字符串常量), 甚至在现代编译器的作用下, 循环体中的代码片段很可能被深度优化(是否还记得NEMU中的pattern_decode()), 使得评估结果虚高, 无法客观反映系统的性能

对于toy project来说, SPEC CPU的程序有点过于真实了: 一方面它们的规模很大, 即使在x86真机中也需要花费小时量级的时间来运行; 另一方面它们需要运行在Linux环境中, 这意味着我们首先需要设计一个能正确启动Linux的CPU, 然后才能运行SPEC CPU这个benchmark. 我们希望:

- 规模不算太大, 在模拟器甚至在RTL仿真环境中的执行时间不到2小时
- 可在裸机环境中运行, 无需启动Linux
- 程序具有一定代表性, 不像CoreMark和Dhrystone那样采用合成程序

事实上, `am-kernels`中集成的`microbench`就是一个不错的选择. 一方面, `microbench`提供了多种规模的测试集, 模拟器可以采用`ref`规模, RTL仿真环境可以采用`train`规模; 另一方面, `microbench`作为一个AM程序, 无需启动Linux即可运行; 此外, `microbench`包含10个子项, 覆盖排序, 位操作, 语言解释器, 矩阵计算, 素数生成, A*算法, 最大网络流, 数据压缩, MD5校验和等场景. 因此, 下面默认使用`microbench`的`train`规模作为benchmark

如果处理器的应用场景比较明确, 例如运行超级玛丽, 那么还可以直接把超级玛丽作为benchmark, 相当于把“超级玛丽的游戏体验”作为“运行得好”的标准. 和microbench不同, 超级玛丽是一个不会运行结束的程序, 因此可以采用FPS作为量化指标来评估, 而不是采用运行时间

## 寻找性能瓶颈

### 性能公式和优化方向
可以把程序的运行时间分解成以下3个因子:

```
        time      inst     cycle     time
perf = ------- = ------ * ------- * -------
        prog      prog      inst     cycle
```

性能优化的目标是减少程序的运行时间, 也即减小其中的每个因子, 这也揭示了性能优化的三个方向.

#### 程序执行的指令数量(即动态指令数)
- 修改程序, 采用更优的算法.
- 采用更优的编译优化策略. 以gcc为例, 除了使用-O3, -Ofast等通用的优化等级参数外, 还可以对目标程序进行针对性的调参. gcc中与生成代码质量相关的编译选项有大约600多个, 选择合适的编译选项, 能大幅优化程序的动态指令数
- 设计并使用行为更复杂的指令集. 我们知道, CISC指令集包含行为较复杂的指令, 若编译器使用这些复杂指令, 则可以降低动态指令数. 另外, 也可以在处理器中添加自定义的专用指令, 并让程序使用这些专用指令.

#### 平均每条指令执行所需的周期数, CPI, IPC
提升平均每周期所执行的指令数. 这个指标反映了处理器的微结构设计效果, 强大的处理器通过每周期能执行更多的指令. 因此, 通常通过优化处理器的微结构设计来提升IPC, 从而使处理器更快地完成程序的执行. 微结构设计的优化又有不同的方向, 我们将在下文简单讨论

#### 每个周期的时间
提升单位时间内的周期数, 即电路的频率. 可能的优化措施包括:
- 优化数字电路的前端设计, 减小关键路径的逻辑延迟
- 优化数字电路的后端设计, 减小关键路径的走线延迟

如果可以量化地评估上述3个因子, 我们就能更好地评估上述三个优化方向的潜力, 从而指导我们找到性能瓶颈. 幸运的是, 这些指标其实不难获取:
- 动态指令数可在仿真环境中直接统计
- 有了动态指令数, 再统计周期数, 即可计算出IPC
- 电路的频率可查看综合器的报告获取

### 统计IPC
在仿真环境中统计IPC:
- `cycle_count` 统计总时钟周期数, 每一次上升沿计一次数
- `inst_count` 统计总的指令数, 取指的valid每拉高一次计一次数
- 分别计算 IPC 与 CPI

### 简单处理器的性能模型
为了找到性能瓶颈, 我们需要分析IPC受哪些因素的影响. 将处理器划分成前端和后端, 其中, 前端包括取指和译码, 剩余的模块属于后端, 负责真正执行指令并改变处理器状态:

```
       /--- frontend ---\    /-------- backend --------\
                                  +-----+ <--- 2. computation efficiency
                             +--> | FU  | --+
       +-----+     +-----+   |    +-----+   |    +-----+
       | IFU | --> | IDU | --+              +--> | WBU |
       +-----+     +-----+   |    +-----+   |    +-----+
          ^                  +--> | LSU | --+
          |                       +-----+
1. instruction supply                ^
                    3. data supply --+
```

要提升处理器的执行效率, 就需要做到: 

- 处理器前端需要保证指令供给. 如果前端取不到足够的指令, 就无法完全发挥处理器的计算能力. 因为每一条指令的执行都需要先取指, 因此前端的指令供给能力将影响所有指令的执行效率.
- 处理器后端需要保证计算效率和数据供给
    - 对于大部分计算类指令, 其执行效率取决于相应功能单元的计算效率. 例如, 乘除法指令的执行效率还受乘除法器的计算效率的影响. 类似的还有浮点执行和浮点处理器单元FPU等
    - 对于访存类指令, 其执行效率取决于LSU的访存效率. 特别地, 对于load指令, 处理器需要等待存储器返回数据, 然后才能将数据写回寄存器堆. 这意味着, load指令的执行效率取决于LSU和存储器的数据供给能力. store指令则比较特殊, 因为store指令不需要写入寄存器堆, 原则上处理器不必等待数据完全写入存储器. 在高性能处理器中, 通常会设计一个store buffer部件, 处理器将store指令的信息写入store buffer后, 即认为store执行结束, 后续由store buffer部件控制将数据真正写入存储器中. 当然, 这增加了处理器设计的复杂度, 例如load指令还需要检查最新的数据是否在store buffer中
    
那么, 我们应该如何量化地评估处理器的指令供给, 计算效率和数据供给呢? 换句话说, 我们真正想了解的是处理器在运行指定的benchmark时, IFU和LSU等模块有没有全速工作. 为此, 我们又需要收集更多的信息.

### 性能事件和性能计数器
为了量化地评估处理器的指令供给, 计算效率和数据供给, 我们需要进一步理解影响它们的细致因素. 以指令供给为例, 能直接反映指令供给能力的, 就是IFU是否取到了指令. 为此, 我们可以把"IFU取到指令"看作一个事件, 来统计这个事件发生的频次, 如果这个事件经常发生, 指令供给能力就强; 否则, 指令供给能力就弱. 这些事件称为性能事件(performance event), 通过它们, 我们可以将性能模型中一些较为抽象的性能指标转化为电路上的具体事件. 类似地, 我们还可以统计"LSU取到数据"这个事件发生的频次, 来衡量数据供给能力的强弱; 统计"EXU完成计算"这个事件发生的频次, 来衡量计算效率的高低.

要统计性能事件发生的频次, 我们只需要在硬件中添加一些计数器, 在检测到性能事件发生时, 计数器的值就加1. 这些计数器称为性能计数器(performance counter). 有了性能计数器, 我们就可以观察"程序在处理器上运行的时间都花在哪里"了, 相当于对处理器内部做 profiling. 要在电路上检测性能事件的发生并不难, 我们可以利用总线机制的握手信号进行检测. 例如, IFU取指的R通道握手时, 表示IFU接收到AXI总线返回的数据, 从而完成一次取指操作, 因此当R通道握手时, 我们就可以让相应的性能计数器加1.

### 添加性能计数器
在NPC中添加一些性能计数器, 至少包含如下性能事件的性能计数器:

- IFU取到指令
- LSU取到数据
- 译码出各种类别的指令, 如计算类指令, 访存指令, CSR指令等

实现上在处理器内部添加一个计数器模块pmu，对事件进行统计，并通过宏来控制是否例化; 在仿真环境中实现程序执行完后收集pmu的计数器值并打印; 实现后运行microbench的test规模, 收集性能计数器的结果, 如果实现正确, 语义相近的不同性能计数器的统计结果应存在一致性. 例如译码得到的不同类别的指令的总数, 应与IFU取到指令的数量一致, 也与动态指令数一致

相比于事件发生, 有时候我们更关心事件什么时候不发生, 以及为什么不发生. 例如, 其实我们更关心IFU什么时候取不到指令, 以及IFU为什么取不到指令, 梳理其中的缘由有助于我们理解指令供给的瓶颈在哪里, 从而为提升处理器的指令供给提供指导. 我们可以把"事件不发生"定义成一个新的事件, 并为新事件添加性能计数器:

- 每种类别的指令占多少比例? 它们各自平均需要执行多少个周期?
- IFU取不到指令的原因有哪些? 这些原因导致IFU取不到指令的几率分别是多少: (当前原因只有后续译码执行访存卡住)
- LSU的平均访存延迟是多少: (读60个cycle, 写30个cycle)

性能计数器的trace: 前面介绍的性能计数器的使用方式都是在仿真结束后输出并分析. 如果我们每周期都输出性能计数器的值, 我们就能得到性能计数器的trace, 借助一些绘图工具(如python的matplotlib绘图库), 我们可以绘制出性能计数器的值随时间变化的曲线, 将仿真过程中性能计数器的变化过程可视化, 从而帮助我们更好地判断性能计数器的变化过程是否符合预期

### 阿姆达尔定律 (Amdahl's law)
性能计数器可以为处理器微结构的优化提供量化指导. 那么, 性能瓶颈究竟在哪里? 哪些优化工作是值得做的呢? 优化工作的预期性能收益是多少? 我们需要在开展具体的优化之前就回答这些问题, 以帮助我们规避那些预期性能收益很低的优化工作, 从而将更多时间投入到收益高的优化工作中. 这听上去像一个预测未来的工作, 但Amdahl's law可以告诉我们答案

```
The overall performance improvement gained by optimizing a single part
of a system is limited by the fraction of time that the improved part
is actually used.

优化系统中某部分所获得的总体性能收益, 受限于那部分实际使用的时间占比
```

假设系统某部分实际使用的时间占比是`p`, 该部分在优化后的加速比是`s`, 则整个系统的加速比为`f(s) = 1 / (1 - p + p / s)`, 这就是Amdahl's law的公式表示

#### 根据性能计数器寻找合适的性能瓶颈
根据性能计数器的统计结果, 尝试挖掘一些潜在的优化对象(以 test 规模的 microbench 为例, **优化方案的选择一定要基于负载的运行情况**):

```
[/home/xinchen/ysyx/npc/csrc/tiktok.c:52 pmu_display] ************ Performance Monitor ************
[/home/xinchen/ysyx/npc/csrc/tiktok.c:53 pmu_display] Total cycle count = 13998424
[/home/xinchen/ysyx/npc/csrc/tiktok.c:54 pmu_display]    - A(alu) type count         = 603912(0.043)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:55 pmu_display]    - B(branch) type count      = 267790(0.019)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:56 pmu_display]    - C(csr) type count         = 1(0.000)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:57 pmu_display]    - Memory load type count    = 4384117(0.313)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:58 pmu_display]    - Memory store type count   = 1735473(0.124)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:59 pmu_display]    - Front end: fetch count    = 6439823(0.460)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:60 pmu_display] Total insts count = 567308
[/home/xinchen/ysyx/npc/csrc/tiktok.c:61 pmu_display]    - A(alu) type count         = 301956(0.532)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:62 pmu_display]    - B(branch) type count      = 133895(0.236)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:63 pmu_display]    - C(csr) type count         = 1(0.000)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:64 pmu_display]    - Memory load type count    = 72022(0.127)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:65 pmu_display]    - Memory store type count   = 59434(0.105)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:66 pmu_display] CPI = 24.675 (IPC = 0.040527)
[/home/xinchen/ysyx/npc/csrc/tiktok.c:67 pmu_display]    - A(alu) type         = 2.000
[/home/xinchen/ysyx/npc/csrc/tiktok.c:68 pmu_display]    - B(branch) type      = 2.000
[/home/xinchen/ysyx/npc/csrc/tiktok.c:69 pmu_display]    - C(csr) type         = 1.000
[/home/xinchen/ysyx/npc/csrc/tiktok.c:70 pmu_display]    - Memory load type    = 60.872
[/home/xinchen/ysyx/npc/csrc/tiktok.c:71 pmu_display]    - Memory store type   = 29.200
[/home/xinchen/ysyx/npc/csrc/tiktok.c:73 pmu_display] *********************************************
```
- 从时间占比上来看, 优化取指和 lsu load 是收益最大的
- 利用Amdahl's law估算它们能获取的理论收益:
    1. 优化取指的理论收益: `1/(1-0.460)=1.85`
    2. 优化laod的理论收益: `1/(1-0.313)=1.46`

### 校准访存延迟
如果仿真环境的行为和真实芯片越接近, 评估结果的误差就越小, 在性能计数器的指导下开展的优化所取得的性能提升就越真实: 之前的ysyxSoC环境是假设处理器和各种外设运行在同一频率下: verilator仿真的一个周期, 既是处理器中的一个周期, 也是外设中的一个周期. 但实际上并非如此: 受电气特性的影响, 外设通常只能运行在低频率, 例如SDRAM颗粒通常只能运行在100MHz左右, 过高的频率会导致时序违例, 使得SDRAM颗粒无法正确工作; 但另一方面, 使用先进工艺的处理器通常能够运行在更高的频率.

为了获得更准确的仿真结果, 以指导我们进行更有效的优化, 我们需要对访存延迟进行校准(calibration). 校准的方式有两种, 一种是使用支持多个时钟域的仿真器, 例如VCS或ICARUS verilog. 和采用周期精确模型方式实现的verilator不同, 这种仿真器采用事件队列模型的方式实现, 把Verilog中的每次计算都看作事件, 并且可以维护事件的延迟, 从而可以正确地维护多时钟域中不同模块在不同频率下工作时每次计算的顺序. 不过为了维护事件队列模型, 这种仿真器的运行速度通常要低于verilator

校准的第二种方式是修改RTL代码, 在ysyxSoC中插入一个延迟模块, 负责将请求延迟若干周期, 来模拟设备在低频率运行的效果, 使得NPC等待的周期数与其将来在高频率运行时所等待的周期数接近. 这种方式的实现不算复杂, 而且可以使用较快的verilator来仿真, 我们选择这种方式. 此外, 这种方式也适用于FPGA. 延迟模块需要等待的周期数与设备服务请求所花费的时间有关, 并不是一个固定的常数, 因此需要在延迟模块中动态计算. 假设请求在设备中花费了k个周期, 处理器和设备的频率比是r(应有r >= 1), 那么延迟模块中需要计算出处理器所需等待的周期数`c = k * r`. 为了进行这一动态计算, 我们又需要考虑两个问题:

1. 如何低开销地实现乘法
2. 如果r是小数, 如何实现小数的乘法

例如yosys-sta项目报告的频率是550MH, 那么`r = 550 / 100 = 5.5`, 但如果把5.5按5来计算, 一个在设备端花费6周期请求将会在处理器端引入3个周期的误差, 对高速运行的CPU来说误差太大, 误差的积累会明显地影响性能计数器的值, 从而进一步影响优化的决策. 考虑到ysyxSoC的代码不参与综合和流片, 其实我们可以用一些简单的方法解决问题, 例如用`*`来计算乘法的结果, 用定点数来表示小数

首先我们先考虑r是整数时, 如何实现乘法. 既然延迟模块本身也需要等待设备的回复, 等待的时间正好是请求在设备中花费的周期数k, 那干脆让延迟模块在等待的每个周期中对一个计数器进行累加, 每周期加r即可. 对于给定的处理器频率和设备频率, r是个定值, 因此可以直接硬编码到RTL代码中. 在延迟模块收到设备的回复后, 就进入等待状态, 每周期让计数器减1, 减到0时再将请求回复给上游.

然后我们来考虑r是小数的情况. 既然小数部分不方便处理, 直接截断又会引入较大误差, 我们可以想办法将小数部分并入整数部分进行累加. 事实上, 我们可以引入一个放大系数s, 累加时每周期往计数器加`r * s`, 即累加结束时, 计数器的值为`y = r * s * k`, 然后在进入等待状态前, 将计数器更新为`y / s`即可. 因为s是一个常数, 因此`r * s`的结果也可以直接硬编码到RTL中, 当然这里的`r * s`很有可能还不是整数, 这里我们将其截断为整数, 虽然这理论上这仍然会引入一定的误差, 但我们可以证明误差比之前小很多. 不过y的值是动态计算的, 不能硬编码到RTL中, 因此对于一般的s, `y / s`需要计算除法. 你应该很快想到, 我们可以取一些特殊的s, 来简化这一计算的过程! 通过这种方式, 我们可以把误差减少到原来的`1/s`, 即原来在累加阶段累积的误差达到s时, 在这种新方法下的误差才增加1

#### 实现校准访存延迟
回顾当前的ysyxSoC, 其中SDRAM采用APB接口, 因此我们需要实现一个APB的延迟模块. ysyxSoC中已经包含一个APB延迟模块的框架, 并集成到APB Xbar的上游, 可捕捉所有APB访问请求, 包括SDRAM的访问请求:

- 在`ysyxSoC/perip/amba/apb_delayer.v`中实现相应代码
- 为了实现APB延迟模块, 根据APB协议的定义, 梳理出一个APB事务何时开始, 何时结束. 假设一个APB事务从`t0`时刻开始, 设备端在`t1`时刻返回APB的回复, APB延迟模块在`t1'`时刻向上游返回APB的回复, 则应有等式`(t1 - t0) * r = t1' - t0`
- 关于r的取值, 假设设备运行在100MHz的环境下, 可以根据yosys-sta的综合报告计算出r. 至于s, 理论上当然是越大越好, 不过你只要选择一个实际中够用的s即可(实际选择了10). 实现后, 尝试取不同的r, 在波形中观察上述等式是否成立

#### 重新寻找优化瓶颈
添加延迟模块后, 重新运行测试并收集性能计数器的统计结果, 然后根据Amdahl's law寻找性能瓶颈:

- 依然采用 test 规模的 microbench, core 频率假设为 1GHz, device 频率假设为 100MHz
- 从时间占比上来看, 优化取指和 lsu load 是收益最大的
- 利用Amdahl's law估算它们能获取的理论收益:
    1. 优化取指的理论收益: `1/(1-0.418)=1.71`
    2. 优化laod的理论收益: `1/(1-0.420)=1.72`

#### 评估NPC的性能
添加延迟模块后, 运行 microbench 的 train 规模测试, 记录各种性能数据, 包括主频信息和各种性能计数器.

校准访存延迟后, 在ysyxSoC中运行microbench的train规模测试预计需要花费数小时, 但我们将得到与流片环境非常接近的性能数据. 后续可以在每次添加一个特性后, 就重新评估并记录性能数据, 来帮助梳理每一个特性带来的性能收益

#### 提升功能测试的效率
将校准访存延迟后的ysyxSoC仿真环境用于性能评估是很合适的, 但这一环境的仿真效率明显低于之前的 riscv32e-ysyxsoc: 在注释 `vsrc/inc/defines.svh` 中 `PMU_ON` 和 `csrc/common.h` 与 `CONFIG_PMU` 后, 即可用于功能测试